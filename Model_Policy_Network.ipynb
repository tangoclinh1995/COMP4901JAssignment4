{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based RL\n",
    "In this exercise you will implement a policy and model network which work in tandem to solve the CartPole reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are we going to accomplish this in Tensorflow? We are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agentâ€™s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries and starting CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.version_info.major > 2:\n",
    "    xrange = range\n",
    "del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "H = 8 # number of Hidden layer neurons\n",
    "learning_rate = 1e-2\n",
    "gamma = 0.99 # Discount factor for reward\n",
    "decay_rate = 0.99 # Decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # Resume from previous checkpoint?\n",
    "\n",
    "model_bs = 3 # Batch size when learning from model\n",
    "real_bs = 3 # Batch size when learning from real environment\n",
    "\n",
    "# Nodel initialization\n",
    "D = 4 # Input dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "observations = tf.placeholder(tf.float32, [None, 4], name = \"input_x\")\n",
    "\n",
    "W1 = tf.get_variable(\n",
    "    \"W1\",\n",
    "    shape = [4, H],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "layer1 = tf.nn.relu(tf.matmul(observations, W1))\n",
    "\n",
    "W2 = tf.get_variable(\n",
    "    \"W2\",\n",
    "    shape=[H, 1],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "\n",
    "score = tf.matmul(layer1, W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "input_y = tf.placeholder(tf.float32, [None, 1], name = \"input_y\")\n",
    "advantages = tf.placeholder(tf.float32, name = \"reward_signal\")\n",
    "\n",
    "adam = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "W1Grad = tf.placeholder(tf.float32, name = \"batch_grad1\")\n",
    "W2Grad = tf.placeholder(tf.float32, name = \"batch_grad2\")\n",
    "batchGrad = [W1Grad, W2Grad]\n",
    "\n",
    "################################################################################\n",
    "# TODO: Implement the loss function.                                           #\n",
    "# This sends the weights in the direction of making actions that gave good     #\n",
    "# advantage (reward overtime) more likely, and actions that didn't less likely.#\n",
    "################################################################################\n",
    "\n",
    "logLikelihood = tf.log(\n",
    "    (1 - input_y) * probability\n",
    "    + input_y * (1 - probability)\n",
    ")\n",
    "\n",
    "loss = -tf.reduce_mean(logLikelihood)\n",
    "\n",
    "################################################################################\n",
    "#                                 END OF YOUR CODE                             #\n",
    "################################################################################\n",
    "\n",
    "newGrads = tf.gradients(loss, tvars)\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Network\n",
    "Here we implement a multi-layer neural network that predicts the next observation, reward, and done state from a current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mH = 256 # Model layer size\n",
    "\n",
    "# input_data = tf.placeholder(tf.float32, [None, 5])  # Redundant ???\n",
    "\n",
    "with tf.variable_scope(\"rnnlm\"):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [mH, 50])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [50])\n",
    "\n",
    "previous_state = tf.placeholder(tf.float32, [None, 5], name = \"previous_state\")\n",
    "\n",
    "W1M = tf.get_variable(\n",
    "    \"W1M\",\n",
    "    shape = [5, mH],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "B1M = tf.Variable(tf.zeros([mH]), name = \"B1M\")\n",
    "\n",
    "layer1M = tf.nn.relu(tf.matmul(previous_state, W1M) + B1M)\n",
    "\n",
    "W2M = tf.get_variable(\n",
    "    \"W2M\",\n",
    "    shape = [mH, mH],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "B2M = tf.Variable(tf.zeros([mH]), name = \"B2M\")\n",
    "\n",
    "layer2M = tf.nn.relu(tf.matmul(layer1M, W2M) + B2M)\n",
    "\n",
    "wO = tf.get_variable(\n",
    "    \"wO\",\n",
    "    shape = [mH, 4],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "\n",
    "wR = tf.get_variable(\n",
    "    \"wR\",\n",
    "    shape = [mH, 1],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "\n",
    "wD = tf.get_variable(\n",
    "    \"wD\",\n",
    "    shape = [mH, 1],\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    ")\n",
    "\n",
    "bO = tf.Variable(tf.zeros([4]), name = \"bO\")\n",
    "bR = tf.Variable(tf.zeros([1]), name = \"bR\")\n",
    "bD = tf.Variable(tf.ones([1]), name=\"bD\")\n",
    "\n",
    "predicted_observation = tf.matmul(layer2M, wO, name = \"predicted_observation\") + bO\n",
    "predicted_reward = tf.matmul(layer2M, wR, name = \"predicted_reward\") + bR\n",
    "predicted_done = tf.sigmoid(tf.matmul(layer2M, wD, name = \"predicted_done\") + bD)\n",
    "\n",
    "true_observation = tf.placeholder(tf.float32, [None, 4], name = \"true_observation\")\n",
    "true_reward = tf.placeholder(tf.float32, [None, 1], name = \"true_reward\")\n",
    "true_done = tf.placeholder(tf.float32, [None, 1], name = \"true_done\")\n",
    "\n",
    "predicted_state = tf.concat([predicted_observation, predicted_reward, predicted_done], 1)\n",
    "\n",
    "observation_loss = tf.square(true_observation - predicted_observation)\n",
    "\n",
    "reward_loss = tf.square(true_reward - predicted_reward)\n",
    "\n",
    "done_loss = tf.multiply(predicted_done, true_done) + tf.multiply(1 - predicted_done, 1 - true_done)\n",
    "done_loss = -tf.log(done_loss)\n",
    "\n",
    "model_loss = tf.reduce_mean(observation_loss + done_loss + reward_loss)\n",
    "\n",
    "modelAdam = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "updateModel = modelAdam.minimize(model_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resetGradBuffer(gradBuffer):\n",
    "    for ix, grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    return gradBuffer\n",
    "\n",
    "\n",
    "\n",
    "def discount_rewards(r):\n",
    "    ################################################################################\n",
    "    # TODO: Implement the discounted rewards function                              #\n",
    "    # Return discounted rewards weighed by gamma. Each reward will be replaced     #\n",
    "    # with a weight reward that involves itself and all the other rewards occuring #\n",
    "    # after it. The later the reward after it happens, the less effect it has on   #\n",
    "    # the current rewards's discounted reward                                      #\n",
    "    # Hint: [r0, r1, r2, ..., r_N] will look someting like:                        #\n",
    "    #       [(r0 + r1*gamma^1 + ... r_N*gamma^N), (r1 + r2*gamma^1 + ...), ...]    #\n",
    "    ################################################################################\n",
    "\n",
    "    result = np.zeros_like(r, dtype = np.float32)\n",
    "\n",
    "    cumm = 0.0        \n",
    "    for i in xrange(result.shape[0] - 1, -1, -1):\n",
    "        cumm = cumm * gamma + r[i, 0]\n",
    "        result[i, 0] = cumm        \n",
    "    \n",
    "    return result        \n",
    "    \n",
    "    ################################################################################\n",
    "    #                                 END OF YOUR CODE                             #\n",
    "    ################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "# This function uses our model to produce a new state when given a previous state and action\n",
    "# xs: Set of states\n",
    "def stepModel(sess, xs, action):\n",
    "    # Just feed the last state in the set xs\n",
    "    toFeed = np.reshape(\n",
    "        np.hstack([xs[-1][0], np.array(action)]),\n",
    "        [1, 5]\n",
    "    )\n",
    "    \n",
    "    myPredict = sess.run([predicted_state], feed_dict = {previous_state: toFeed})\n",
    "    \n",
    "    reward = myPredict[0][:, 4]\n",
    "    \n",
    "    observation = myPredict[0][:, 0: 4]\n",
    "    observation[:, 0] = np.clip(observation[:, 0], -2.4, 2.4)\n",
    "    observation[:, 2] = np.clip(observation[:, 2], -0.4, 0.4)\n",
    "    \n",
    "    doneP = np.clip(myPredict[0][:, 5], 0, 1)\n",
    "        \n",
    "    if doneP > 0.1 or len(xs) >= 300:\n",
    "        done = True\n",
    "    else:\n",
    "        done = False\n",
    "        \n",
    "    return observation, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Policy and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World Perf: Episode 4. Reward 17.000000. action: 0.000000. mean reward 17.000000.\n",
      "World Perf: Episode 7. Reward 23.000000. action: 1.000000. mean reward 17.060000.\n",
      "World Perf: Episode 10. Reward 31.000000. action: 0.000000. mean reward 17.199400.\n",
      "World Perf: Episode 13. Reward 28.666667. action: 0.000000. mean reward 17.314073.\n",
      "World Perf: Episode 16. Reward 32.333333. action: 1.000000. mean reward 17.464265.\n",
      "World Perf: Episode 19. Reward 19.333333. action: 1.000000. mean reward 17.482956.\n",
      "World Perf: Episode 22. Reward 24.000000. action: 0.000000. mean reward 17.548126.\n",
      "World Perf: Episode 25. Reward 27.333333. action: 0.000000. mean reward 17.645978.\n",
      "World Perf: Episode 28. Reward 26.333333. action: 0.000000. mean reward 17.732852.\n",
      "World Perf: Episode 31. Reward 24.666667. action: 1.000000. mean reward 17.802190.\n",
      "World Perf: Episode 34. Reward 26.000000. action: 1.000000. mean reward 17.884168.\n",
      "World Perf: Episode 37. Reward 26.000000. action: 0.000000. mean reward 17.965327.\n",
      "World Perf: Episode 40. Reward 22.333333. action: 1.000000. mean reward 18.009007.\n",
      "World Perf: Episode 43. Reward 33.000000. action: 1.000000. mean reward 18.158917.\n",
      "World Perf: Episode 46. Reward 20.333333. action: 1.000000. mean reward 18.180661.\n",
      "World Perf: Episode 49. Reward 15.666667. action: 1.000000. mean reward 18.155521.\n",
      "World Perf: Episode 52. Reward 20.000000. action: 1.000000. mean reward 18.173966.\n",
      "World Perf: Episode 55. Reward 20.333333. action: 1.000000. mean reward 18.195559.\n",
      "World Perf: Episode 58. Reward 27.000000. action: 0.000000. mean reward 18.283604.\n",
      "World Perf: Episode 61. Reward 23.666667. action: 0.000000. mean reward 18.337434.\n",
      "World Perf: Episode 64. Reward 24.333333. action: 0.000000. mean reward 18.397393.\n",
      "World Perf: Episode 67. Reward 32.000000. action: 0.000000. mean reward 18.533419.\n",
      "World Perf: Episode 70. Reward 15.333333. action: 0.000000. mean reward 18.501419.\n",
      "World Perf: Episode 73. Reward 19.000000. action: 0.000000. mean reward 18.506404.\n",
      "World Perf: Episode 76. Reward 33.333333. action: 0.000000. mean reward 18.654674.\n",
      "World Perf: Episode 79. Reward 20.333333. action: 1.000000. mean reward 18.671460.\n",
      "World Perf: Episode 82. Reward 18.666667. action: 0.000000. mean reward 18.671412.\n",
      "World Perf: Episode 85. Reward 26.666667. action: 0.000000. mean reward 18.751365.\n",
      "World Perf: Episode 88. Reward 44.666667. action: 0.000000. mean reward 19.010518.\n",
      "World Perf: Episode 91. Reward 42.000000. action: 0.000000. mean reward 19.240413.\n",
      "World Perf: Episode 94. Reward 30.666667. action: 0.000000. mean reward 19.354675.\n",
      "World Perf: Episode 97. Reward 13.666667. action: 0.000000. mean reward 19.297795.\n",
      "World Perf: Episode 100. Reward 23.333333. action: 0.000000. mean reward 19.338150.\n",
      "World Perf: Episode 103. Reward 19.000000. action: 0.000000. mean reward 19.334769.\n",
      "World Perf: Episode 106. Reward 27.000000. action: 0.000000. mean reward 19.439833.\n",
      "World Perf: Episode 109. Reward 18.000000. action: 1.000000. mean reward 19.268635.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1bb40badaf1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Start displaying environment once performance is acceptably high.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreward_sum\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdrawFromModel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mrendering\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mrendering\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programs\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\WinPython-64bit-3.5.4.0Qt5\\python-3.5.4.amd64\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flip'"
     ]
    }
   ],
   "source": [
    "xs, drs, ys, ds = [], [], [], []\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "real_episodes = 1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "batch_size = real_bs\n",
    "\n",
    "drawFromModel = False # When set to True, will use model for observations\n",
    "trainTheModel = True # Whether to train the model\n",
    "\n",
    "trainThePolicy = False # Whether to train the policy\n",
    "\n",
    "switch_point = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    rendering = False\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    observation = env.reset()\n",
    "    x = observation\n",
    "    \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "    \n",
    "    while episode_number <= 5000:\n",
    "        # Start displaying environment once performance is acceptably high.\n",
    "        if (reward_sum / batch_size > 150 and drawFromModel == False) or rendering == True: \n",
    "            env.render()\n",
    "            rendering = True\n",
    "            \n",
    "        x = np.reshape(observation, [1, 4])\n",
    "\n",
    "        tfprob = sess.run(probability, feed_dict = {observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "\n",
    "        # Record various intermediates (needed later for backprop)\n",
    "        xs.append(x)\n",
    "        \n",
    "        y = 1 if action == 0 else 0    # \"Fake\" label\n",
    "        ys.append(y)\n",
    "        \n",
    "        # Step the  model or real environment and get new measurements\n",
    "        if drawFromModel == False:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "        else:\n",
    "            observation, reward, done = stepModel(sess, xs, action)\n",
    "                \n",
    "        reward_sum += reward\n",
    "        \n",
    "        ds.append(done * 1)\n",
    "        \n",
    "        # Record reward (has to be done after we call step() to get reward for previous action)\n",
    "        drs.append(reward) \n",
    "\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            \n",
    "            if drawFromModel == False: \n",
    "                real_episodes += 1\n",
    "\n",
    "            # Stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            epd = np.vstack(ds)\n",
    "            \n",
    "            xs, drs, ys, ds = [], [], [], [] # Reset array memory\n",
    "            \n",
    "            if trainTheModel == True:                \n",
    "                ################################################################################\n",
    "                # TODO: Run the model network and compute predicted_state                      #\n",
    "                # Output: 'pState'                                                             #\n",
    "                ################################################################################\n",
    "                \n",
    "                # Note: action & y is ALWAYS opposite\n",
    "                epXandAction = np.hstack([epx, 1 - epy])\n",
    "                \n",
    "                pState = sess.run(predicted_state, feed_dict = {previous_state: epXandAction})\n",
    "                state_nextsAll = np.hstack([epx, epr, epd])\n",
    "                \n",
    "                sess.run(updateModel, feed_dict = {\n",
    "                    previous_state: epXandAction,\n",
    "                    true_observation: np.vstack([epx[1: ], epx[-1] + 1e-6]),\n",
    "                    true_reward: epr,\n",
    "                    true_done: epd\n",
    "                })\n",
    "                \n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################                \n",
    "\n",
    "            if trainThePolicy == True:\n",
    "                \n",
    "                ################################################################################\n",
    "                # TODO: Run the policy network and compute newGrads                            #\n",
    "                # Output: 'tGrad'                                                              #\n",
    "                ################################################################################\n",
    "                \n",
    "                discounted_epr = discount_rewards(epr)\n",
    "                discounted_epr -= np.mean(discounted_epr)\n",
    "                discounted_epr /= np.std(discounted_epr)\n",
    "                \n",
    "                tGrad = sess.run(newGrads, feed_dict = {\n",
    "                    observations: epx,\n",
    "                    input_y: epy,\n",
    "                    advantages: discounted_epr\n",
    "                })\n",
    "                \n",
    "                ################################################################################\n",
    "                #                                 END OF YOUR CODE                             #\n",
    "                ################################################################################\n",
    "                \n",
    "                # If gradients becom too large, end training process\n",
    "                if np.sum(tGrad[0] == tGrad[0]) == 0:\n",
    "                    break\n",
    "                    \n",
    "                for ix, grad in enumerate(tGrad):\n",
    "                    gradBuffer[ix] += grad\n",
    "                \n",
    "            if switch_point + batch_size == episode_number:\n",
    "                switch_point = episode_number\n",
    "                \n",
    "                if trainThePolicy == True:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO:                                                                        #\n",
    "                    # (1) Run the policy network and update gradients                              #\n",
    "                    # (2) Reset gradBuffer to 0                                                    #\n",
    "                    ################################################################################\n",
    "                    \n",
    "                    sess.run(updateGrads, feed_dict = {\n",
    "                        W1Grad: gradBuffer[0],\n",
    "                        W2Grad: gradBuffer[1]\n",
    "                    })\n",
    "                    \n",
    "                    gradBuffer = resetGradBuffer(gradBuffer)\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "\n",
    "                running_reward = reward_sum if running_reward is None else (running_reward * 0.99 + reward_sum * 0.01)\n",
    "                \n",
    "                if drawFromModel == False:\n",
    "                    print(\"World Perf: Episode %d. Reward %f. action: %f. mean reward %f.\" % (\n",
    "                        real_episodes,\n",
    "                        reward_sum / real_bs,\n",
    "                        action,\n",
    "                        running_reward / real_bs\n",
    "                    ))\n",
    "                    \n",
    "                    if reward_sum / batch_size > 200:\n",
    "                        break\n",
    "                        \n",
    "                reward_sum = 0\n",
    "\n",
    "                # Once the model has been trained on 100 episodes\n",
    "                if episode_number > 100:\n",
    "                    \n",
    "                    ################################################################################\n",
    "                    # TODO: Alternating between training the policy from the model and training    #\n",
    "                    # the model from the real environment.                                         #\n",
    "                    ################################################################################                 \n",
    "                    \n",
    "                    drawFromModel = not drawFromModel\n",
    "                    trainTheModel = not trainTheModel\n",
    "                    \n",
    "                    trainThePolicy = not trainThePolicy                    \n",
    "                    \n",
    "                    ################################################################################\n",
    "                    #                                 END OF YOUR CODE                             #\n",
    "                    ################################################################################\n",
    "            \n",
    "            if drawFromModel == True:\n",
    "                observation = np.random.uniform(-0.1, 0.1, [4]) # Generate reasonable starting point\n",
    "                batch_size = model_bs\n",
    "            else:\n",
    "                observation = env.reset()\n",
    "                batch_size = real_bs\n",
    "                \n",
    "print(real_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking model representation\n",
    "Here we can examine how well the model is able to approximate the true environment after training. The green line indicates the real environment, and the blue indicates model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 12))\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(6, 2, 2 * i + 1)\n",
    "    plt.plot(pState[:, i])\n",
    "    plt.subplot(6, 2, 2 * i + 1)\n",
    "    plt.plot(state_nextsAll[:, i])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
